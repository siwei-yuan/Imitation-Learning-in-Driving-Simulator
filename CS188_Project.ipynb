{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNtgNN3FF1q6",
        "outputId": "5daf6d1d-2ed3-439f-f637-4fa465eecea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YXp7HwhtdkU7F11QTZXsTPrT9jq3LDAn\n",
            "To: /content/dataset.tar.gz\n",
            "100% 28.5M/28.5M [00:00<00:00, 51.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip3 install --upgrade gdown --quiet\n",
        "#!gdown 16GYHdSWS3iMYwMPv5FpeDZN2rH7PR0F2 # this is the file ID of miniplaces dataset\n",
        "!gdown 1YXp7HwhtdkU7F11QTZXsTPrT9jq3LDAn\n",
        "# back-up commands (try the following it previous file id is overload)\n",
        "# !gdown 1CyIQOJienhNITwGcQ9h-nv8z6GOjV2HX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tB8tvZhIF1q9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "import urllib.request\n",
        "\n",
        "def setup(folder_name='Week7'):\n",
        "  # Let's make our assignment directory\n",
        "  CS188_path = './'\n",
        "  os.makedirs(os.path.join(CS188_path, 'Week7', 'data'), exist_ok=True)\n",
        "  # Now, let's specify the assignment path we will be working with as the root.\n",
        "  root_dir = os.path.join(CS188_path, 'Week7')\n",
        "  # Open the tar.gz file\n",
        "  tar = tarfile.open(\"dataset.tar.gz\", \"r:gz\")\n",
        "  # Extract the file \"./Assignment2/data\" folder\n",
        "  total_size = sum(f.size for f in tar.getmembers())\n",
        "  with tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Extracting tar.gz file\") as pbar:\n",
        "      for member in tar.getmembers():\n",
        "          tar.extract(member, os.path.join(root_dir, 'data'))\n",
        "          pbar.update(member.size)\n",
        "  # Close the tar.gz file\n",
        "  tar.close()\n",
        "  return root_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx-2pvciF1q-",
        "outputId": "07c4979c-01eb-4617-92c3-43405a791047"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting tar.gz file: 100%|██████████| 28.4M/28.4M [00:00<00:00, 89.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "root_dir = setup(folder_name='Week7')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4MyTph9F1q_"
      },
      "source": [
        "### Define the data transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1v9w2JugF1rA"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define data transformation\n",
        "# You can copy your data transform from Assignment2. \n",
        "# Notice we are resize images to 128x128 instead of 64x64.\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXrmPXGEF1rB"
      },
      "source": [
        "### Define the dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KoHHge5YF1rB"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class Metadrive(Dataset):\n",
        "    def __init__(self, root_dir, split, transform=None):\n",
        "        \n",
        "        assert split in ['train', 'val', 'test']\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.filenames = []\n",
        "\n",
        "        count = 0\n",
        "        # Iterate directory\n",
        "        for path in os.listdir(os.path.join(root_dir, 'dataset', split)):\n",
        "          if os.path.isfile(os.path.join(root_dir, 'dataset', split, path)):\n",
        "            self.filenames.append(path)\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        Label returned is of the form: tuple((float)steering, (float)acceleration, (float)velocity)\n",
        "        '''\n",
        "        image = None\n",
        "        label = None\n",
        "\n",
        "        image_path = os.path.join(self.root_dir, 'dataset', self.split, self.filenames[idx])\n",
        "        image = Image.open(image_path)\n",
        "        if self.transform:\n",
        "          image = self.transform(image)\n",
        "        label = self.filenames[idx][1:-5]\n",
        "        label = label.split(', ')\n",
        "        steering = float(label[0])*100\n",
        "        acceleration = float(label[1])*10\n",
        "        return image, torch.Tensor([steering, acceleration])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_th2HTIF1rC"
      },
      "source": [
        "### Define the train method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kfX-uwDDF1rC"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
        "    \"\"\"\n",
        "    Train the MLP classifier on the training set and evaluate it on the validation set every epoch.\n",
        "    \n",
        "    Args:\n",
        "        model (MLP): MLP classifier to train.\n",
        "        train_loader (torch.utils.data.DataLoader): Data loader for the training set.\n",
        "        val_loader (torch.utils.data.DataLoader): Data loader for the validation set.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer to use for training.\n",
        "        criterion (callable): Loss function to use for training.\n",
        "        device (torch.device): Device to use for training.\n",
        "        num_epochs (int): Number of epochs to train the model.\n",
        "    \"\"\"\n",
        "    # Place model on device\n",
        "    model = model.to(device)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        \n",
        "        # Use tqdm to display a progress bar during training\n",
        "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}') as pbar:\n",
        "            for inputs, labels in train_loader:\n",
        "                # Move inputs and labels to device\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                # Zero out gradients\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                # Compute the logits and loss\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, labels)\n",
        "                \n",
        "                # Backpropagate the loss\n",
        "                loss.backward()\n",
        "                \n",
        "                # Update the weights\n",
        "                optimizer.step()\n",
        "                \n",
        "                # Update the progress bar\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "          \n",
        "        scheduler.step()\n",
        "        \n",
        "        # Evaluate the model on the validation set\n",
        "        avg_loss = evaluate(model, val_loader, criterion, device)\n",
        "        print(f'Validation set: Average loss = {avg_loss:.4f}')\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate the MLP classifier on the test set.\n",
        "    \n",
        "    Args:\n",
        "        model (MLP): MLP classifier to evaluate.\n",
        "        test_loader (torch.utils.data.DataLoader): Data loader for the test set.\n",
        "        criterion (callable): Loss function to use for evaluation.\n",
        "        device (torch.device): Device to use for evaluation.\n",
        "        \n",
        "    Returns:\n",
        "        float: Average loss on the test set.\n",
        "        float: Accuracy on the test set.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        total_loss = 0.0\n",
        "        \n",
        "        for inputs, labels in test_loader:\n",
        "            # Move inputs and labels to device\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Compute the logits and loss\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            \n",
        "    # Compute the average loss and accuracy\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    \n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Pob9aqlkF1rF"
      },
      "outputs": [],
      "source": [
        "# Also, seed everything for reproducibility\n",
        "# code from https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964#file-seed_everything-py\n",
        "def seed_everything(seed: int):\n",
        "    import random, os\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMTQ7y7lF1rF",
        "outputId": "ae205c7d-146a-4fc3-8c35-5137cc56b735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please set GPU via Edit -> Notebook Settings.\n"
          ]
        }
      ],
      "source": [
        "# Define the device to use for training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device == torch.device('cuda'):\n",
        "    print(f'Using device: {device}. Good to go!')\n",
        "else:\n",
        "    print('Please set GPU via Edit -> Notebook Settings.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sstf5tHfHK1M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torchvision.models as models\n",
        "class Resnet(nn.Module):\n",
        "    def __init__(self, mode='linear',pretrained=True):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        use the resnet18 model from torchvision models. Remember to set pretrained as true\n",
        "        \n",
        "        mode has three options:\n",
        "        1) features: to extract features only, we do not want the last fully connected layer of \n",
        "            resnet18. Use nn.Identity() to replace this layer.\n",
        "        2) linear: For this model, we want to freeze resnet18 features, then train a linear \n",
        "            classifier which takes the features before FC (again we do not want \n",
        "            resnet18 FC). And then write our own FC layer: which takes in the features and \n",
        "            output scores of size 100 (because we have 100 categories).\n",
        "            Because we want to freeze resnet18 features, we have to iterate through parameters()\n",
        "            of our model, and manually set some parameters to requires_grad = False\n",
        "            Or use other methods to freeze the features\n",
        "        3) finetune: Same as 2), except that we we do not need to freeze the features and\n",
        "           can finetune on the pretrained resnet model.\n",
        "        \"\"\"\n",
        "        self.resnet = None\n",
        "        self.resnet = models.resnet18(pretrained = pretrained)\n",
        "\n",
        "        if mode == 'feature':\n",
        "          self.resnet.fc = nn.Identity()\n",
        "        \n",
        "        if mode == 'linear':\n",
        "          for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "          self.resnet.fc = nn.Linear(512, 2)\n",
        "\n",
        "        if mode == 'finetune':\n",
        "          for param in self.resnet.parameters():\n",
        "            param.requires_grad = True\n",
        "          self.resnet.fc = nn.Linear(512, 2)\n",
        "    #####################################################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        return x\n",
        "    \n",
        "    def to(self,device):\n",
        "        return self.resnet.to(device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY9uLr3-IhHJ",
        "outputId": "b640a604-a680-49bc-fb2b-7689e9699359"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 15/15 [00:17<00:00,  1.17s/it, loss=16.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 19.7127\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 15/15 [00:17<00:00,  1.19s/it, loss=12.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 15.8314\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 15/15 [00:18<00:00,  1.21s/it, loss=13.4]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 16.4831\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|██████████| 15/15 [00:17<00:00,  1.15s/it, loss=15.6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 15.9780\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|██████████| 15/15 [00:16<00:00,  1.11s/it, loss=12.8]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 15.9938\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|██████████| 15/15 [00:16<00:00,  1.12s/it, loss=15.5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 16.1759\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|██████████| 15/15 [00:17<00:00,  1.19s/it, loss=9.62]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 16.0860\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|██████████| 15/15 [00:17<00:00,  1.14s/it, loss=10.4]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 15.9274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|██████████| 15/15 [00:16<00:00,  1.12s/it, loss=14.4]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 16.0530\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|██████████| 15/15 [00:16<00:00,  1.12s/it, loss=11.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 16.0662\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|██████████| 15/15 [00:16<00:00,  1.12s/it, loss=11.1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 16.1033\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|██████████| 15/15 [00:16<00:00,  1.13s/it, loss=10.4]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 16.1025\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|██████████| 15/15 [00:17<00:00,  1.15s/it, loss=10.2]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 16.2191\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20: 100%|██████████| 15/15 [00:16<00:00,  1.13s/it, loss=12.1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 16.1137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20: 100%|██████████| 15/15 [00:17<00:00,  1.14s/it, loss=13.5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 16.1533\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20: 100%|██████████| 15/15 [00:16<00:00,  1.13s/it, loss=11.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 16.1749\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20: 100%|██████████| 15/15 [00:20<00:00,  1.35s/it, loss=12.1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 16.0678\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20: 100%|██████████| 15/15 [00:16<00:00,  1.12s/it, loss=13.6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 16.1732\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/20: 100%|██████████| 15/15 [00:17<00:00,  1.14s/it, loss=13.6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 16.0973\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/20: 100%|██████████| 15/15 [00:16<00:00,  1.12s/it, loss=9.52]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 16.0146\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "model = Resnet(mode='linear',pretrained=True)\n",
        "\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(), \n",
        "    lr=0.001, \n",
        "    momentum=0.9)\n",
        "\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "# Define the dataset and data transform with flatten functions appended\n",
        "# data_dir = os.path.join('.', 'dataset')\n",
        "data_dir = '.'\n",
        "\n",
        "train_dataset = Metadrive(\n",
        "    root_dir=data_dir, split='train', \n",
        "    transform=data_transform)\n",
        "\n",
        "val_dataset = Metadrive(\n",
        "    root_dir=data_dir, split='val', \n",
        "    transform=data_transform)\n",
        "\n",
        "# Define the batch size and number of workers\n",
        "batch_size = 64\n",
        "num_workers = 0\n",
        "\n",
        "# Define the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "PATH = \"model.pt\"\n",
        "torch.save({\n",
        "    # 'epoch': EPOCH,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    # 'optimizer_state_dict': optimizer.state_dict(),\n",
        "    # 'loss': LOSS,\n",
        "}, PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resnet(\n",
            "  (resnet): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:You may using too large buffer! The width is 128, and length is 128.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "__init__() missing 1 required positional argument: 'model'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6280\\1137346113.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_track_vehicle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpert_takeover\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6280\\1137346113.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMetaDriveEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHELP_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpert_takeover\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\hp\\desktop\\cv\\metadrive\\metadrive\\envs\\base_env.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, force_seed)\u001b[0m\n\u001b[0;32m    354\u001b[0m                 \u001b[1;34m\"singleton of MetaDrive and restart your program.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             )\n\u001b[1;32m--> 356\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_top_down_renderer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_top_down_renderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\hp\\desktop\\cv\\metadrive\\metadrive\\engine\\base_engine.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m                 \u001b[1;31m# The scene will be generated from replay manager in only reset replay mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m             \u001b[0mmanager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[1;31m# lm = process_memory()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\hp\\desktop\\cv\\metadrive\\metadrive\\manager\\agent_manager.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infinite_agents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"num_agents\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_allow_respawn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"allow_respawn\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         self.episode_created_agents = self._get_vehicles(\n\u001b[0m\u001b[0;32m    115\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target_vehicle_configs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         )\n",
            "\u001b[1;32mc:\\users\\hp\\desktop\\cv\\metadrive\\metadrive\\manager\\agent_manager.py\u001b[0m in \u001b[0;36m_get_vehicles\u001b[1;34m(self, config_dict)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mpolicy_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_cls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\hp\\desktop\\cv\\metadrive\\metadrive\\manager\\base_manager.py\u001b[0m in \u001b[0;36madd_policy\u001b[1;34m(self, object_id, policy_class, *policy_args, **policy_kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mpolicy_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpolicy_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mpolicy_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpolicy_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\hp\\desktop\\cv\\metadrive\\metadrive\\engine\\base_engine.py\u001b[0m in \u001b[0;36madd_policy\u001b[1;34m(self, object_id, policy_class, *args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object_policies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobject_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_episode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'model'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Please feel free to run this script to enjoy a journey by keyboard!\n",
        "Remember to press H to see help message!\n",
        "Note: This script require rendering, please following the installation instruction to setup a proper\n",
        "environment that allows popping up an window.\n",
        "\"\"\"\n",
        "\n",
        "print(model)\n",
        "\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from metadrive import MetaDriveEnv\n",
        "from metadrive.constants import HELP_MESSAGE\n",
        "\n",
        "from metadrive.component.algorithm.blocks_prob_dist import PGBlockDistConfig\n",
        "from metadrive.component.map.base_map import BaseMap\n",
        "from metadrive.component.map.pg_map import MapGenerateMethod\n",
        "\n",
        "from rgb_policy import RGBPolicy\n",
        "\n",
        "\n",
        "PRINT_IMG = False\n",
        "SAMPLING_INTERVAL = 10\n",
        "\n",
        "\n",
        "config = dict(\n",
        "    # controller=\"joystick\",\n",
        "    use_render=True,\n",
        "    offscreen_render=True,\n",
        "    manual_control=True,\n",
        "    traffic_density=0,\n",
        "    environment_num=100,\n",
        "    random_agent_model=False,\n",
        "    start_seed=random.randint(0, 1000),\n",
        "    vehicle_config = dict(image_source=\"rgb_camera\", \n",
        "                            rgb_camera= (128 , 128),\n",
        "                            stack_size=1),\n",
        "    block_dist_config=PGBlockDistConfig,\n",
        "    random_lane_width=True,\n",
        "    random_lane_num=False,\n",
        "    map_config={\n",
        "        BaseMap.GENERATE_TYPE: MapGenerateMethod.BIG_BLOCK_SEQUENCE,\n",
        "        BaseMap.GENERATE_CONFIG: \"SCCCSCCC\",  # it can be a file path / block num / block ID sequence\n",
        "        BaseMap.LANE_WIDTH: 3.5,\n",
        "        BaseMap.LANE_NUM: 1,\n",
        "        \"exit_length\": 50,\n",
        "    },\n",
        "    agent_policy=RGBPolicy\n",
        ")\n",
        "env = MetaDriveEnv(config)\n",
        "try:\n",
        "    o = env.reset()\n",
        "    print(HELP_MESSAGE)\n",
        "    env.vehicle.expert_takeover = True\n",
        "    assert isinstance(o, dict)\n",
        "    print(\"The observation is a dict with numpy arrays as values: \", {k: v.shape for k, v in o.items()})\n",
        "    #for i in range(1, 31):\n",
        "    for i in range(1, 100000000000):\n",
        "        o, r, d, info = env.step([0, 0])\n",
        "        \n",
        "        # Action space is of form (float, float) -> Tuple\n",
        "        # It encodes the necessary info about vehicle movement\n",
        "        action_space = (info['steering'], info['acceleration'], info['velocity'])\n",
        "        print(action_space)\n",
        "\n",
        "        # Change PRINT_IMG to True if recording FPV\n",
        "        # Change step_size to set sampling rate\n",
        "        # Image saved is named by the action\n",
        "        if PRINT_IMG and i%SAMPLING_INTERVAL == 0:\n",
        "            img = np.squeeze(o['image'][:,:,:,0]*255).astype(np.uint8)\n",
        "            img = img[...,::-1].copy() # convert to rgb\n",
        "            img = Image.fromarray(img)\n",
        "            root_dir = os.path.join(os.getcwd(), 'dataset', 'val')\n",
        "            img_path = os.path.join(root_dir, str(action_space) + \".png\")\n",
        "            img.save(str(img_path))\n",
        "\n",
        "        env.render(\n",
        "            text={\n",
        "                \"Auto-Drive (Switch mode: T)\": \"on\" if env.current_track_vehicle.expert_takeover else \"off\",\n",
        "            }\n",
        "        )\n",
        "        if d and info[\"arrive_dest\"]:\n",
        "            env.reset()\n",
        "            env.current_track_vehicle.expert_takeover = True\n",
        "except Exception as e:\n",
        "    raise e\n",
        "finally:\n",
        "    env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b302cdd1e032ee910f5c889c3360c28564c92ad4f326fc3102e39fbe47faee66"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
